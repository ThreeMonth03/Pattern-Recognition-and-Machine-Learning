{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZQD8NqPhKyBP"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score # Please note that this is the only sklearn function that can be utilized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV1MHt_VTg9f"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a1vkTOD6K5Nj"
      },
      "outputs": [],
      "source": [
        "# Load the train/val/test dataset\n",
        "\n",
        "df_train = pd.DataFrame(pd.read_csv(\"./PR_HW3_Train.csv\"))\n",
        "df_val   = pd.DataFrame(pd.read_csv(\"./PR_HW3_Val.csv\"))\n",
        "df_test  = pd.DataFrame(pd.read_csv(\"./PR_HW3_Test.csv\"))\n",
        "\n",
        "X_train = df_train[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
        "y_train = df_train[\"Target\"].to_numpy()\n",
        "\n",
        "X_val = df_val[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
        "y_val = df_val[\"Target\"].to_numpy()\n",
        "\n",
        "X_test = df_test[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
        "y_test = df_test[\"Target\"].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MJcktFIuK78Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(800, 7)\n",
            "(800,)\n",
            "(800, 7)\n",
            "(800,)\n",
            "(800, 7)\n",
            "(800,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa3hnJ9sTkvh"
      },
      "source": [
        "# Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5e_nviB8LAK8"
      },
      "outputs": [],
      "source": [
        "def gini(sequence):\n",
        "    labels, counts = np.unique(sequence, return_counts=True)\n",
        "    probs = counts/ float(np.shape(sequence)[0])\n",
        "    gini = 1- np.sum(probs * probs)\n",
        "    return gini    \n",
        "\n",
        "\n",
        "def entropy(sequence):\n",
        "    labels, counts = np.unique(sequence, return_counts=True)\n",
        "    probs = counts/ float(np.shape(sequence)[0])\n",
        "    entropy = - np.sum(probs * np.log2(probs))\n",
        "    return entropy    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_HJA_108LF_G"
      },
      "outputs": [],
      "source": [
        "class Tree():\n",
        "    \"\"\"\n",
        "        You can add/change any variables/methods to meet your need.\n",
        "    \"\"\"   \n",
        "    def __init__(self, true_branch=None, false_branch=None, feature_index=0, threshold=0, depth=0):    \n",
        "        self.left_child = false_branch        \n",
        "        self.right_child = true_branch\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.depth = depth\n",
        "        self.is_leaf = 0\n",
        "        self.predict_feature = 0\n",
        "        self.gain = 0\n",
        "        \n",
        "    def set_node(self, feature_index=None, threshold=None, depth= 0, gain=0):\n",
        "        self.left_child = Tree()\n",
        "        self.right_child = Tree()\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.depth = depth\n",
        "        self.gain = gain\n",
        "\n",
        "    def set_leaf(self, predict_feature, depth= 0):\n",
        "        self.predict_feature = predict_feature\n",
        "        self.is_leaf = 1\n",
        "        \n",
        "    def get_branch(self):\n",
        "      return self.right_child, self.left_child"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O_vgKCIwLdS0"
      },
      "outputs": [],
      "source": [
        "class DecisionTree():\n",
        "    def __init__(self, criterion='gini', max_depth=None, max_features=None):       \n",
        "        \"\"\"\n",
        "            You can add/change any variables/methods to meet your need.\n",
        "        \"\"\"\n",
        "        self.criterion = criterion\n",
        "        self.importance = {}\n",
        "        self.tree = Tree()\n",
        "        self.max_features = max_features\n",
        "        \n",
        "        if max_depth is None:\n",
        "            self.max_depth = 1e9\n",
        "        else:\n",
        "            self.max_depth = max_depth\n",
        "            \n",
        "    def criterion_calculation(self,sequence):\n",
        "        if np.shape(sequence)[0] == 0:\n",
        "            return 0\n",
        "        if self.criterion == 'gini':\n",
        "            return gini(sequence)\n",
        "        if self.criterion == 'entropy':\n",
        "            return entropy(sequence)\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        if self.max_features == None:\n",
        "            self.max_features = np.shape(X)[1]\n",
        "        self.build_tree(X, y, 0, self.tree)\n",
        "        self.countImportance()\n",
        "\n",
        "    def build_tree(self, X, y, depth, curr_node):\n",
        "        if depth < self.max_depth:\n",
        "            critierion_gain, condition, threshold, True_X, True_y, False_X, False_y = self.find_best_split(X,y)\n",
        "            if critierion_gain == 0:\n",
        "                values, counts = np.unique(y, return_counts=True)\n",
        "                most_class = values[np.argmax(counts)]\n",
        "                curr_node.set_leaf(predict_feature = most_class, depth= depth)             \n",
        "            else:\n",
        "                curr_node.set_node(feature_index= condition, threshold= threshold, depth= depth, gain= critierion_gain)\n",
        "                true_branch, false_branch = curr_node.get_branch()\n",
        "                self.build_tree(True_X, True_y, depth+1, true_branch)\n",
        "                self.build_tree(False_X, False_y, depth+1, false_branch)\n",
        "        \n",
        "        if depth == self.max_depth:\n",
        "            values, counts = np.unique(y, return_counts=True)\n",
        "            most_class = values[np.argmax(counts)]\n",
        "            curr_node.set_leaf(predict_feature = most_class, depth= depth)\n",
        "\n",
        "    def find_best_split(self,X,y):\n",
        "        total_critierion = self.criterion_calculation(y)\n",
        "        min_splitted_critierion = float('inf')\n",
        "        best_threshold = 0\n",
        "        best_condition = 0\n",
        "        dataset_size = np.shape(X)[0]\n",
        "        num_feature = np.shape(X)[1]\n",
        "        choose_feature = np.random.choice(num_feature, self.max_features, replace=False)\n",
        "        for col in choose_feature:\n",
        "            col_value = np.unique(X[:,col])\n",
        "            if(np.shape(col_value)[0] == 1):\n",
        "                min_splitted_critierion = total_critierion\n",
        "                break\n",
        "            \n",
        "            col_mean_value = ((col_value + np.roll(col_value, 1))/2)[1:]\n",
        "            for threshold in col_mean_value:\n",
        "                TF_table = np.where(X[:,col]>=threshold, True, False)\n",
        "                splitted_critierion = ((dataset_size - TF_table.sum()) * self.criterion_calculation(y[np.invert(TF_table)]) + TF_table.sum()*self.criterion_calculation(y[TF_table]))/dataset_size\n",
        "          \n",
        "                if splitted_critierion < min_splitted_critierion:\n",
        "                    min_splitted_critierion = splitted_critierion\n",
        "                    best_condition = col\n",
        "                    best_threshold = threshold\n",
        "            \n",
        "        best_TF_table = np.where(X[:,best_condition]>=best_threshold, True, False)     \n",
        "        return total_critierion - min_splitted_critierion, best_condition, best_threshold, X[best_TF_table], y[best_TF_table], X[np.invert(best_TF_table)], y[np.invert(best_TF_table)]\n",
        "    \n",
        "    def predict(self, X):\n",
        "        num_data = np.shape(X)[0]\n",
        "        prediction = np.zeros(num_data)\n",
        "        for i in range(num_data):\n",
        "            prediction[i] = self.find_the_leaf(self.tree, X[i])\n",
        "        return prediction\n",
        "    \n",
        "    def find_the_leaf(self, curr_node, X_row):\n",
        "        if curr_node.is_leaf == 1:\n",
        "            return curr_node.predict_feature\n",
        "        else:\n",
        "            child_node = None\n",
        "            if X_row[curr_node.feature_index]>= curr_node.threshold:\n",
        "                child_node = curr_node.right_child\n",
        "            else:\n",
        "                child_node = curr_node.left_child\n",
        "            return self.find_the_leaf(child_node, X_row)\n",
        "        \n",
        "    def countImportance(self):\n",
        "        self.traverse_tree(self.tree, mode=\"calculate_importance\")\n",
        "\n",
        "    def traverse_tree(self,curr_node, mode=\"print\", str=\"root\"):\n",
        "        if(curr_node.is_leaf == 0):\n",
        "            if(mode == \"print\"):\n",
        "                self.print_node_info(curr_node, str)\n",
        "                self.traverse_tree(curr_node.left_child,mode,str+\"->left\")\n",
        "                self.traverse_tree(curr_node.right_child,mode,str+\"->right\")\n",
        "            \n",
        "            if(mode == \"calculate_importance\"):\n",
        "                self.update_importance(curr_node)\n",
        "                self.traverse_tree(curr_node.left_child,mode)\n",
        "                self.traverse_tree(curr_node.right_child,mode)            \n",
        "\n",
        "        else:\n",
        "            if(mode == \"print\"):\n",
        "                self.print_leaf_info(curr_node, str)\n",
        "            \n",
        "    def print_node_info(self,curr_node, str):\n",
        "        print(\"str : \",str)\n",
        "        print(\"curr_node.feature_index : \",curr_node.feature_index)\n",
        "        print(\"curr_node.threshold : \",curr_node.threshold)\n",
        "        print(\"curr_node.depth : \",curr_node.depth)\n",
        "        print(\"curr_node.is_leaf : \",curr_node.is_leaf)\n",
        "        print(\"curr_node.gain : \",curr_node.gain)\n",
        "        print(\"-------------------------\")\n",
        "        \n",
        "    def print_leaf_info(self,curr_node, str):\n",
        "        print(\"str : \",str)\n",
        "        print(\"curr_node.is_leaf : \",curr_node.is_leaf)\n",
        "        print(\"curr_node.predict_feature : \",curr_node.predict_feature)\n",
        "        print(\"-------------------------\")\n",
        "    \n",
        "    def update_importance(self, curr_node):\n",
        "        feature_index = curr_node.feature_index\n",
        "        if feature_index not in self.importance:\n",
        "            self.importance[feature_index] = 1\n",
        "        else:\n",
        "            self.importance[feature_index] += 1\n",
        "            \n",
        "    def print_importance(self):\n",
        "        #for key in self.importance:\n",
        "        #    print(key,\" : \",self.importance[key])\n",
        "        print(self.importance)\n",
        "        \n",
        "    def draw_importance(self, labelList, title):\n",
        "        feature_num = len(self.importance)\n",
        "        np_importance = np.zeros(feature_num)\n",
        "        for i in range(feature_num):\n",
        "            np_importance[i] = self.importance[i]\n",
        "        index = np.argsort(np_importance)\n",
        "        sort_labellist = []\n",
        "        for i in range(feature_num):\n",
        "            sort_labellist.append(labelList[index[i]])\n",
        "        plt.title(title)\n",
        "        plt.barh(sort_labellist, np_importance[index])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BE8wu0MGN_H-"
      },
      "outputs": [],
      "source": [
        "class RandomForest():\n",
        "    \"\"\"\n",
        "        You can add/change any variables/methods to meet your need.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=10, max_features=None, boostrap=True, criterion='gini', max_depth=None):\n",
        "        \n",
        "        self.n_estimators = n_estimators\n",
        "        self.boostrap = boostrap\n",
        "        self.criterion = criterion\n",
        "        self.forest = []\n",
        "        self.max_depth = max_depth\n",
        "        if max_features==None:\n",
        "            self.max_features = None\n",
        "        else:\n",
        "            self.max_features = np.int64(max_features)\n",
        "        \n",
        "    def fit(self, X, y, feature_random = False): \n",
        "        dataset_size = np.shape(X)[0]        \n",
        "            \n",
        "        for i in range(self.n_estimators):\n",
        "            #Initialize tree       \n",
        "            tree_i = DecisionTree(criterion=self.criterion, max_features= self.max_features, max_depth=self.max_depth)\n",
        "            #Train the tree\n",
        "            if self.boostrap == True:\n",
        "                boostrapping_index = np.random.choice(dataset_size, dataset_size, replace=True)\n",
        "                train_X = X[boostrapping_index]\n",
        "                train_y = y[boostrapping_index]\n",
        "            else:\n",
        "                train_X = X\n",
        "                train_y = y           \n",
        "            tree_i.fit(train_X[:], train_y, sample_weight=None)\n",
        "            #Append tree into forest\n",
        "            self.forest.append(tree_i)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # majority vote\n",
        "        num_data = np.shape(X)[0]\n",
        "        total_prediction = np.zeros((len(self.forest), num_data))\n",
        "        prediction = np.zeros((num_data))\n",
        "        \n",
        "        for i, tree in enumerate(self.forest):\n",
        "            total_prediction[i] = tree.predict(X)\n",
        "        total_prediction = np.transpose(total_prediction, (1, 0))\n",
        "        \n",
        "        for i in range(num_data):\n",
        "            values, counts = np.unique(total_prediction[i], return_counts=True)\n",
        "            prediction[i] = values[np.argmax(counts)]\n",
        "            \n",
        "        return prediction        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPUsaCh2T9Fs"
      },
      "source": [
        "# Questions for Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zSB-Uqp4OaaX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['+' '+' '+' '+' '+' '-']: entropy = 0.6500224216483541\n",
            "['+' '+' '+' '-' '-' '-']: entropy = 1.0\n",
            "['+' '-' '-' '-' '-' '-']: entropy = 0.6500224216483541\n",
            "\n",
            "['+' '+' '+' '+' '+' '-']: gini index = 0.2777777777777777\n",
            "['+' '+' '+' '-' '-' '-']: gini index = 0.5\n",
            "['+' '-' '-' '-' '-' '-']: gini index = 0.2777777777777777\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Q1\n",
        "ex1 = np.array([\"+\", \"+\", \"+\", \"+\", \"+\", \"-\"])\n",
        "ex2 = np.array([\"+\", \"+\", \"+\", \"-\", \"-\", \"-\"])\n",
        "ex3 = np.array([\"+\" ,\"-\", \"-\", \"-\", \"-\", \"-\"])\n",
        "\n",
        "print(f\"{ex1}: entropy = {entropy(ex1)}\\n{ex2}: entropy = {entropy(ex2)}\\n{ex3}: entropy = {entropy(ex3)}\\n\")\n",
        "print(f\"{ex1}: gini index = {gini(ex1)}\\n{ex2}: gini index = {gini(ex2)}\\n{ex3}: gini index = {gini(ex3)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G_t9N9fnOdon"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q2-1 max_depth=3:  0.73125\n"
          ]
        }
      ],
      "source": [
        "# For Q2-1, validation accuracy should be higher than or equal to 0.73\n",
        "\n",
        "np.random.seed(0) # You may adjust the seed number in all the cells\n",
        "\n",
        "dt_depth3 = DecisionTree(criterion='gini', max_features=None, max_depth=3)\n",
        "dt_depth3.fit(X_train, y_train, sample_weight=None)\n",
        "acc = accuracy_score(y_val, dt_depth3.predict(X_val))\n",
        "\n",
        "print(\"Q2-1 max_depth=3: \", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "T0HcgzMdjHRP"
      },
      "outputs": [],
      "source": [
        "\"\"\" Do Not Modify Below \"\"\"\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier as SK_DecisionTreeClassifier\n",
        "\n",
        "sk_dt = SK_DecisionTreeClassifier(criterion='gini', max_depth=3)\n",
        "sk_dt.fit(X_train, y_train)\n",
        "sk_acc = accuracy_score(y_val, sk_dt.predict(X_val))\n",
        "\n",
        "assert round(acc, 3) == round(sk_acc, 3), \"Because the Decision Tree without any trick has a fixed answer, your accuracy should be the same as sklearn, otherwise your implementation might have some problems\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SjCPMr-eQ7jn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q2-2 max_depth=10:  0.8675\n"
          ]
        }
      ],
      "source": [
        "# For Q2-2, validation accuracy should be higher than or equal to 0.85\n",
        "\n",
        "np.random.seed(0)\n",
        "dt_depth10 = DecisionTree(criterion='gini', max_features=None, max_depth=10)\n",
        "dt_depth10.fit(X_train, y_train, sample_weight=None)\n",
        "\n",
        "print(\"Q2-2 max_depth=10: \", accuracy_score(y_val,  dt_depth10.predict(X_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iTbxGPrbO2jT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q3-1 criterion='gini':  0.73125\n"
          ]
        }
      ],
      "source": [
        "# For Q3-1, validation accuracy should be higher than or equal to 0.73\n",
        "\n",
        "np.random.seed(0)\n",
        "dt_gini = DecisionTree(criterion='gini', max_features=None, max_depth=3)\n",
        "dt_gini.fit(X_train, y_train, sample_weight=None)\n",
        "\n",
        "print(\"Q3-1 criterion='gini': \", accuracy_score(y_val, dt_gini.predict(X_val)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1XG7eAKUQ-YU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q3-2 criterion='entropy':  0.7725\n"
          ]
        }
      ],
      "source": [
        "# For Q3-2, validation accuracy should be higher than or equal to 0.77\n",
        "\n",
        "np.random.seed(0)\n",
        "dt_entropy = DecisionTree(criterion='entropy', max_features=None, max_depth=3)\n",
        "dt_entropy.fit(X_train, y_train, sample_weight=None)\n",
        "\n",
        "print(\"Q3-2 criterion='entropy': \", accuracy_score(y_val, dt_entropy.predict(X_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "joE89xabPsXg"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# For Q4\n",
        "\n",
        "# Use simply counting to get the feature importance: dt_depth10.importance\n",
        "labelList=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']\n",
        "title = 'Feature Importance'\n",
        "dt_depth10.draw_importance(labelList, title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg97qz_xUGfP"
      },
      "source": [
        "# Questions for Random Rorest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SlrdIW1ERJ8F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q6-1 n_estimators=10:  0.89875\n"
          ]
        }
      ],
      "source": [
        "# For Q5-1, validation accuracy should be higher than or equal to 0.88\n",
        "\n",
        "np.random.seed(0)\n",
        "rf_estimators10 = RandomForest(n_estimators=10, max_features=np.sqrt(X_train.shape[1]), boostrap=True, criterion='gini', max_depth=None)\n",
        "rf_estimators10.fit(X_train, y_train)\n",
        "\n",
        "print(\"Q6-1 n_estimators=10: \", accuracy_score(y_val, rf_estimators10.predict(X_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4qcLuIkbRUfM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q6-1 n_estimators=50:  0.9\n"
          ]
        }
      ],
      "source": [
        "# For Q5-2, validation accuracy should be higher than or equal to 0.89\n",
        "\n",
        "np.random.seed(0)\n",
        "rf_estimators50 = RandomForest(n_estimators=50, max_features=np.sqrt(X_train.shape[1]), boostrap=True, criterion='gini', max_depth=None)\n",
        "rf_estimators50.fit(X_train, y_train)\n",
        "\n",
        "print(\"Q6-1 n_estimators=50: \", accuracy_score(y_val, rf_estimators50.predict(X_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "n-DbniYhRYmM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q7-1 max_features='sqrt':  0.89875\n"
          ]
        }
      ],
      "source": [
        "# For Q6-1, validation accuracy should be higher than or equal to 0.88\n",
        "\n",
        "np.random.seed(0)\n",
        "rf_maxfeature_sqrt = RandomForest(n_estimators=10, max_features=np.sqrt(X_train.shape[1]), boostrap=True, criterion='gini', max_depth=None)\n",
        "rf_maxfeature_sqrt.fit(X_train, y_train)\n",
        "\n",
        "print(\"Q7-1 max_features='sqrt': \", accuracy_score(y_val,  rf_maxfeature_sqrt.predict(X_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PF9yufSaRffn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q7-1 max_features='All':  0.875\n"
          ]
        }
      ],
      "source": [
        "# For Q6-2, validation accuracy should be higher than or equal to 0.86\n",
        "\n",
        "np.random.seed(0)\n",
        "rf_maxfeature_none = RandomForest(n_estimators=10, max_features=None, boostrap=True, criterion='gini', max_depth=None)\n",
        "rf_maxfeature_none.fit(X_train, y_train)\n",
        "\n",
        "print(\"Q7-1 max_features='All': \", accuracy_score(y_val, rf_maxfeature_none.predict(X_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize(data1, data2, data3):\n",
        "    npmax = np.max(data1, axis = 0)\n",
        "    npmin = np.min(data1, axis = 0)\n",
        "    range_ = npmax - npmin\n",
        "    return (data1 - npmin)/range_ , (data2 - npmin)/range_ , (data3 - npmin)/range_\n",
        "\n",
        "def get_feature(data1, data2, data3):\n",
        "    data_num = np.shape(data1)[0]\n",
        "    feature_size = np.shape(data1)[1]\n",
        "    r_data1 = np.zeros((data_num, feature_size + np.int64(feature_size*(feature_size + 1)/2)))\n",
        "    r_data2 = np.zeros((data_num, feature_size + np.int64(feature_size*(feature_size + 1)/2)))\n",
        "    r_data3 = np.zeros((data_num, feature_size + np.int64(feature_size*(feature_size + 1)/2)))\n",
        "    r_data1[:,:feature_size] = data1\n",
        "    r_data2[:,:feature_size] = data2\n",
        "    r_data3[:,:feature_size] = data3\n",
        "    index = feature_size\n",
        "    r_id = np.array([0,1,2,3,5,6])\n",
        "    return data1[:,r_id],data2[:,r_id],data3[:,r_id]\n",
        "           \n",
        "# Load the train/val/test dataset\n",
        "\n",
        "df_train = pd.DataFrame(pd.read_csv(\"./PR_HW3_Train.csv\"))\n",
        "df_val   = pd.DataFrame(pd.read_csv(\"./PR_HW3_Val.csv\"))\n",
        "df_test  = pd.DataFrame(pd.read_csv(\"./PR_HW3_Test.csv\"))\n",
        "\n",
        "X_train = df_train[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
        "y_train = df_train[\"Target\"].to_numpy()\n",
        "\n",
        "X_val = df_val[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
        "y_val = df_val[\"Target\"].to_numpy()\n",
        "\n",
        "X_test = df_test[['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7']].to_numpy()\n",
        "y_test = df_test[\"Target\"].to_numpy()\n",
        "\n",
        "X_train_k_fold = np.concatenate((X_train, X_val), axis=0)\n",
        "y_train_k_fold = np.concatenate((y_train, y_val), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjopdAZqUKbF"
      },
      "source": [
        "# Train your own model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Va4L29gfUPO8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0  iteration:\n",
            "The val of my model :  0.8875\n",
            "1  iteration:\n",
            "The val of my model :  0.934375\n",
            "2  iteration:\n",
            "The val of my model :  0.9\n",
            "3  iteration:\n",
            "The val of my model :  0.90625\n",
            "4  iteration:\n",
            "The val of my model :  0.90625\n",
            "average acc:  0.906875\n",
            "total val acc:  0.96125\n"
          ]
        }
      ],
      "source": [
        "# Build and train your model\n",
        "class k_fold_forest():\n",
        "    def __init__(self, k, n_estimators, max_features, boostrap, criterion, max_depth):\n",
        "        self.k = k\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.boostrap = boostrap\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.forest_list = []\n",
        "    \n",
        "    def fit(self, X_train_k_fold, y_train_k_fold):\n",
        "        average_acc = 0\n",
        "        for i in range(self.k):\n",
        "            X_train_ = np.concatenate((X_train_k_fold[0:np.int64(1600*i/self.k)], X_train_k_fold[np.int64(1600*(i+1)/self.k):1600]), axis=0)\n",
        "            y_train_ = np.concatenate((y_train_k_fold[0:np.int64(1600*i/self.k)], y_train_k_fold[np.int64(1600*(i+1)/self.k):1600]), axis=0)    \n",
        "            X_val_ = X_train_k_fold[np.int64(1600*i/self.k) : np.int64(1600*(i+1)/self.k)]\n",
        "            y_val_ = y_train_k_fold[np.int64(1600*i/self.k) : np.int64(1600*(i+1)/self.k)]\n",
        "            your_model = RandomForest(n_estimators=self.n_estimators, max_features=self.max_features, boostrap=self.boostrap, criterion=self.criterion, max_depth=self.max_depth)\n",
        "            your_model.fit(X_train_, y_train_)\n",
        "            print(i,\" iteration:\")\n",
        "            print(\"The val of my model : \", accuracy_score(y_val_, your_model.predict(X_val_)))\n",
        "            average_acc += accuracy_score(y_val_, your_model.predict(X_val_))\n",
        "            self.forest_list.append(your_model)\n",
        "        average_acc /= self.k \n",
        "        print(\"average acc: \",average_acc)\n",
        "    \n",
        "    def predict(self, X_test_):\n",
        "        num_data = np.shape(X_test_)[0]\n",
        "        voting = np.zeros((self.k, num_data))\n",
        "        prediction = np.zeros((num_data))\n",
        "        \n",
        "        for i,forest in enumerate(self.forest_list):\n",
        "            voting[i] = forest.predict(X_test_)\n",
        "        \n",
        "        voting = np.transpose(voting, (1, 0))\n",
        "        \n",
        "        for i in range(num_data):\n",
        "            values, counts = np.unique(voting[i], return_counts=True)\n",
        "            prediction[i] = values[np.argmax(counts)]       \n",
        "        \n",
        "        return prediction\n",
        "    \n",
        "np.random.seed(0)\n",
        "your_model = k_fold_forest(k = 5,n_estimators=450, max_features=2, boostrap=True, criterion='entropy', max_depth=8)\n",
        "your_model.fit(X_train_k_fold, y_train_k_fold)\n",
        "val_pred = your_model.predict(X_val)\n",
        "\n",
        "acc = accuracy_score(y_val, val_pred)\n",
        "print(\"total val acc: \", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5cmxQjK3Rja9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_pred shape:  (800,)\n"
          ]
        }
      ],
      "source": [
        "test_pred = your_model.predict(X_test)\n",
        "print(\"test_pred shape: \", test_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XCaZ4yFuR34B"
      },
      "outputs": [],
      "source": [
        "# output csv\n",
        "df_test = pd.DataFrame(pd.read_csv(\"./PR_HW3_Test.csv\"))\n",
        "df_test[\"Target\"] = test_pred\n",
        "df_test.to_csv(\"0810749_prediction.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
